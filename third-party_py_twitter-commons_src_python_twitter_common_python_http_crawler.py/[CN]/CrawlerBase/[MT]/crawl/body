def crawl(self, urls, follow_links=False):
    (links, seen) = (set(), set())
    queue = Queue()
    converged = threading.Event()

    def execute():
        while (not converged.is_set()):
            try:
                url = queue.get(timeout=0.1)
            except Empty:
                continue
            if (url not in seen):
                seen.add(url)
                (hrefs, rel_hrefs) = self.execute(url)
                links.update(hrefs)
                if follow_links:
                    for href in rel_hrefs:
                        if (href not in seen):
                            queue.put(href)
            queue.task_done()
    for url in urls:
        queue.put(url)
    for _ in range(self._threads):
        worker = threading.Thread(target=execute)
        worker.daemon = True
        worker.start()
    queue.join()
    converged.set()
    return links
